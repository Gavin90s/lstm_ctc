* MOE

Mobvoi E2E speech recognition (MOE) uses high rank LSTM-CTC based models. The toolkit is inspired by [kaldi](http://kaldi-asr.org/) and [EESEN](https://github.com/srvk/eesen). Data preparation, feature processing and WFST based graph operation is fork from kaldi. LSTM-CTC deep model training is built based on [tensorflow](https://www.tensorflow.org/). WFST based method from EESEN is applied to leverge token, lexicon and language model (TLG) for decoding.

    


